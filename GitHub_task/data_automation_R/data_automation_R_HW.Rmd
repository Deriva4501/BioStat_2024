---
title: "data_automation_R_HW"
output:
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(flextable)
library(RColorBrewer)
library(broom)
library(ggbeeswarm)
library(Hmisc)
library(reshape2)
library(tidymodels)

```

# Чтение данных

В вашем варианте нужно использовать датасет food.

```{r}
data <- read_csv("data/raw/food.csv")
```

# Выведите общее описание данных

```{r}
glimpse(data)
```

# Очистка данных

1) Уберите переменные, в которых пропущенных значений больше 20% или уберите субъектов со слишком большим количеством пропущенных значений. Или совместите оба варианта. Напишите обоснование, почему вы выбрали тот или иной вариант:

**Обоснование**: 

2) Переименуйте переменные в человекочитаемый вид (что делать с пробелами в названиях?);

3) В соответствии с описанием данных приведите переменные к нужному типу (numeric или factor);

4) Отсортируйте данные по возрасту по убыванию;

5) Сохраните в файл outliers.csv субъектов, которые являются выбросами (например, по правилу трёх сигм) — это необязательное задание со звёздочкой;

6) Отфильтруйте датасет так, чтобы остались только Rice и Cookie (переменная Category и есть группирующая);

7) Присвойте получившийся датасет переменной "cleaned_data".

```{r}

cleaned_data <- data %>% select(where(~ mean(is.na(.)) <= 0.2)) %>% 
  filter(rowMeans(is.na(.)) <= 0.2) %>%  # В данном наборе данных отсутствуют пропущенные значения
  rename_with(~ stringi::stri_replace_all_regex 
              (., c("\\.", " ", "^Data_", "^Vitamins_", "^Fat_", "^Major_", "-", "__RAE", "Minerals_"), 
                c("_", "_", "", "", "", "", "", "", ""), 
                vectorize_all = FALSE)) %>% 
  mutate(across(where(is.character), as.factor),
         across(where(is.integer), as.numeric),
         Nutrient_Data_Bank_Number = 
           as.factor(format(Nutrient_Data_Bank_Number))) %>% 
  arrange(desc(`Sugar_Total`)) %>% 
  filter(Category == "Rice" | Category == "Cookie") %>% 
  mutate(Category = droplevels(Category)) # Убираем лишние уровни фактора


# 5 задание
data %>%
  mutate(across(where(is.numeric), 
                ~ (abs(. - mean(., na.rm = TRUE)) > 3 * sd(., na.rm = TRUE)), 
                .names = "outlier_{col}")) %>%
  filter(rowSums(select(., starts_with("outlier_"))) > 0) %>% 
  select(-starts_with("outlier_")) %>% 
  write.csv2(., "outliers.csv", row.names = FALSE)

```

# Сколько осталось переменных?

```{r}

ncol(cleaned_data)

```

# Сколько осталось случаев?

```{r}

nrow(cleaned_data)

```

# Есть ли в данных идентичные строки?

```{r}

sum(duplicated(cleaned_data))

```

# Сколько всего переменных с пропущенными значениями в данных и сколько пропущенных точек в каждой такой переменной?

```{r}

sum(is.na(cleaned_data))

```

# Описательные статистики

## Количественные переменные

1) Рассчитайте для всех количественных переменных для каждой группы (Category):

1.1) Количество значений;

1.2) Количество пропущенных значений;

1.3) Среднее;

1.4) Медиану;

1.5) Стандартное отклонение;

1.6) 25% квантиль и 75% квантиль;

1.7) Интерквартильный размах;

1.8) Минимум;

1.9) Максимум;

1.10) 95% ДИ для среднего - задание со звёздочкой.

```{r}

statistics <- list(
  `Количество значений` = ~length(.x) %>% as.character(),
  `Количество пропущенных значений` = ~sum(is.na(.x)) %>% as.character(),
  `Среднее` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", mean(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `Медиана` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", median(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `Стандартное отклонение` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", sd(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `25% квантиль` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", quantile(.x, 0.25, na.rm = TRUE) %>% round(2) %>% as.character()), 
  `75% квантиль` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", quantile(.x, 0.75, na.rm = TRUE) %>% round(2) %>% as.character()), 
  `Интерквартильный размах` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*",
                                        IQR(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `Минимум` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", min(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `Максимум` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", max(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `95% ДИ для среднего` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", 
                                  {
                                    n = sum(!is.na(.x))
                                    mean_x = mean(.x, na.rm = TRUE)
                                    sd_x = sd(.x, na.rm = TRUE)
                                    error = 1.96 * (sd_x / sqrt(n))
                                    paste0(round(mean_x - error, 2),
                                           " - ",
                                           round(mean_x + error, 2))
                                  })
)

cleaned_data %>% 
  select(Category, where(is.numeric)) %>% 
  group_by(Category) %>% 
  summarise(across(where(is.numeric), statistics, .names = "{.col}&{.fn}")) %>% 
  pivot_longer(!Category) %>% 
  separate(name, into = c("Variable", "Statistic"), sep = "&") %>% 
  rename(Value = value) %>% 
  flextable() %>% 
  autofit() %>% 
  merge_v(j = c("Category", "Variable")) %>% 
  hline(j = c("Category", "Variable"), part = "all") %>% 
  border(border = fp_border_default(color = "black", width = 1), part = "all")

```

## Категориальные переменные

1) Рассчитайте для всех категориальных переменных для каждой группы (Category):

1.1) Абсолютное количество;

1.2) Относительное количество внутри группы;

1.3) 95% ДИ для доли внутри группы - задание со звёздочкой.

```{r}

cleaned_data %>%
  group_by(Category) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(
    Fraction = n / sum(n),  # Доля от общего числа
    CI = map(n, ~prop.test(.x, sum(n), conf.level = 0.95)$conf.int)
  ) %>% 
  mutate(
    CI_lower = map_dbl(CI, 1),
    CI_upper = map_dbl(CI, 2) 
  ) %>% 
  mutate(
    Fraction = round(Fraction, 3),  # Округление до 2 знаков
    CI_lower = round(CI_lower, 3), 
    CI_upper = round(CI_upper, 3)) %>% 
  select(Category, n, Fraction, CI_lower, CI_upper) %>% 
  flextable() %>% 
  set_header_labels(n = "Абсолютное количество",
                    Fraction = "Относительное количество внутри группы",
                    CI_lower = "95% ДИ нижний",
                    CI_upper = "95% ДИ верхний") %>% 
  autofit() %>% 
  border(border = fp_border_default(color = "black", width = 1), part = "all")


```

# Визуализация

## Количественные переменные

1) Для каждой количественной переменной сделайте боксплоты по группам. Расположите их либо на отдельных рисунках, либо на одном, но читаемо;

2) Наложите на боксплоты beeplots - задание со звёздочкой.

3) Раскрасьте боксплоты с помощью библиотеки RColorBrewer.

```{r, fig.width=14, fig.height=10}

var_groups <- cleaned_data %>% 
  select_if(is.numeric) %>% 
  names() %>% 
  split(ceiling(seq_along(.) / 10)) # Создание групп переменных по 10 в каждой

map(var_groups, function(vars) {
  ggplot(cleaned_data %>% 
           select(Category, all_of(vars)) %>% 
           pivot_longer(-Category, names_to = "Variable", values_to = "Values"), 
         aes(x = Category, y = Values, color = Category)) +
    geom_boxplot(outlier.shape = NA, alpha = 0.5) +
    geom_beeswarm(size = 1, alpha = 0.6) +  # Beeplot
    scale_color_manual(values = brewer.pal(n = 3, name = "Set1")[c(1,2)]) +
    facet_wrap(~Variable, scales = "free_y", ncol = 5) + #RColorBrewer
    theme_minimal() +
    theme(text = element_text(size = 14))
})

```

## Категориальные переменные

1) Сделайте подходящие визуализации категориальных переменных. Обоснуйте, почему выбрали именно этот тип.

```{r}

cleaned_data %>%
  group_by(Category) %>%                    
  summarise(Frequency = n()) %>%         
  mutate(Percentage = Frequency / sum(Frequency) * 100) %>%
  ggplot(aes(x = "", y = Frequency, fill = Category)) +
  geom_bar(stat = "identity", width = 1) + 
  geom_text(aes(label = paste0(Frequency, " (", round(Percentage, 1), "%)")), 
            position = position_stack(vjust = 0.5), 
            color = "white") +               
  coord_polar("y") +
  labs(title = "Proportion of Categories", fill = "Category", x = NULL) +
  theme_minimal()  


```

Сначала выбрал барплот, т.к. это оч распространенный и привычный многим тип графика. Потом решил, что круговая диаграмма нагляднее отражает соотношение.

# Статистические оценки

## Проверка на нормальность

1) Оцените каждую переменную на соответствие нормальному распределению с помощью теста Шапиро-Уилка. Какие из переменных являются нормальными и как как вы это поняли?

```{r}

cleaned_data %>% 
  select(Category, where(is.numeric)) %>% 
  group_by(Category) %>% 
  summarise(across(where(is.numeric), ~shapiro.test(.x)$p.value %>% round(4))) %>% 
  pivot_longer(-Category, names_to = "Variable", values_to = "p_value") %>%
  mutate(`Normal distribution` = ifelse(p_value > 0.05, "Да", "Нет")) %>% 
  flextable() %>%
  autofit() %>% 
  merge_v(j = c("Category", "Variable")) %>% 
  hline(j = c("Category", "Variable"), part = "all") %>% 
  border(border = fp_border_default(color = "black", width = 1), part = "all")
  


```

Переменные с p-value > 0.05 считаются нормально распределёнными, поскольку не отвергается нулевая гипотеза о нормальном распределении. 
Нормальное распределение наблюдается по углеводам, жирам и натрию в печенье. 

2) Постройте для каждой количественной переменной QQ-плот. Отличаются ли выводы от теста Шапиро-Уилка? Какой метод вы бы предпочли и почему?

```{r, fig.width=14, fig.height=10}

numeric_vars <- cleaned_data %>%
  select(where(is.numeric)) %>%
  names()

# Разюиение на группы по 10 переменных в каждой
var_groups <- split(numeric_vars, ceiling(seq_along(numeric_vars) / 10))

lapply(var_groups, function(vars) {
  cleaned_data %>%
    select(Category, all_of(vars)) %>%
    pivot_longer(-Category, names_to = "Variable", values_to = "Value") %>%
    ggplot(aes(sample = Value, color = Category)) + 
    stat_qq() +
    stat_qq_line(size = 1) +
    facet_wrap(~Variable, scales = "free", ncol = 5) +
    labs(
      title = "QQ-plots",
      x = "Theoretical quantile",
      y = "Empirical quantile"
    ) +
    theme_minimal() +
    theme(strip.text = element_text(size = 14))
})


```

Согласно тесту Шапиро-Уилка нормальное распределение наблюдается по переменным Carbohydrate, Total_Lipid, Sodium в группе Cookie. Если смотреть на QQ плоты, по этим переменным данные действительно лежат в пределах линии с оч небольшими отклонениями.

Однако, согласно QQ плотам, я бы сказал, что распределение близко к нормальному также по переменным Fiber и Niacin в группе Rice.

3) Ниже напишите, какие ещё методы проверки на нормальность вы знаете и какие у них есть ограничения.

**Напишите текст здесь**

1.Тест Колмогорова-Смирнова
Ограничения:
Менее мощный, чем тест Шапиро-Уилка. Может не обнаружить отклонения от нормальности в случае небольших отклонений.
Не обладает высокой чувствительностью к отклонениям от нормальности, если данные имеют небольшие выборки.
Трудно интерпретировать при большом количестве данных, поскольку его результаты могут быть неточными из-за чувствительности к объему выборки.

2. Тест Лиллиефорса
Модификация теста Колмогорова-Смирнова.
Ограничения:
Это тест, который применяется только к нормальному распределению. Не подходит для данных с другими распределениями.
Также чувствителен к выборке и может не сработать на больших объемах данных.
Когда использовать: Когда параметры нормального распределения неизвестны и необходимо оценить нормальность без предположений о значениях среднего и дисперсии.

3. Тест Андерсона-Дарлинга
Описание: Это более мощный тест, чем тест Колмогорова-Смирнова, и часто используется для проверки нормальности. Он учитывает хвосты распределения, что делает его более чувствительным к отклонениям в этих областях.
Ограничения:
Как и другие тесты, он может быть чувствителен к большому объему данных и не очень эффективен для малых выборок.
Менее известен и реже используется в большинстве стандартных пакетов статистического анализа.
Когда использовать: Если необходимо учитывать хвосты распределения (например, при анализе финансовых данных или экстремальных отклонений).

4. Тест Д’Агостино
Описание: Этот тест основан на оценке асимметрии (скоса) и эксцесса (крутоты) распределения и проверяет гипотезу о нормальности распределения. Он используется для больших выборок.
Ограничения:
Применим для больших выборок, так как он может быть неточным для малых выборок.
Может не справляться с отклонениями от нормальности, которые выражены в хвостах распределения, так как фокусируется на центральной части распределения.
Когда использовать: Для больших выборок, особенно когда необходимо учитывать асимметрию и эксцесс.

## Сравнение групп

1) Сравните группы (переменная **Category**) по каждой переменной (как количественной, так и категориальной). Для каждой переменной выберите нужный критерий и кратко обоснуйте его выбор в комментариях.

```{r}
# С учетом распределения данных и наличия двух групп данных был выбран тест Манна-Уитни
cleaned_data %>%
  select(Category, where(is.numeric)) %>%               
  pivot_longer(-Category, names_to = "Variable", values_to = "Value") %>%  
  group_by(Variable) %>%                               
  summarise(
    test_result = list(wilcox.test(Value ~ Category)),   
    p_value = test_result[[1]]$p.value) %>%
  mutate(
    p_value = ifelse(p_value < 0.001, "<0,001", round(p_value, 3)),  # Форматирование p-значений
    significance = ifelse(p_value < 0.05, "Yes", "No")  # Определение значимости
  ) %>%
  select(Variable, p_value, significance) %>% 
  flextable()


```

# Далее идут **необязательные** дополнительные задания, которые могут принести вам дополнительные баллы в том числе в случае ошибок в предыдущих

## Корреляционный анализ

1) Создайте корреляционную матрицу с визуализацией и поправкой на множественные сравнения. Объясните, когда лучше использовать корреляционные матрицы и в чём минусы и плюсы корреляционных исследований.

```{r}

cleaned_data %>% 
  select_if(is.numeric) %>% 
  as.matrix() %>% 
  rcorr() %>% 
    {
      cor_matr <- .$r
      p_val <- .$P
      p_val_adj <- p.adjust(as.vector(p_val), method = "bonferroni") 
      
      map2(cor_matr, p_val_adj, ~paste0("r = ", round(.x, 2), "\np = ", round(.y, 3))) %>% 
        flatten_chr() %>% 
        matrix(nrow = nrow(cor_matr), ncol = ncol(cor_matr), 
               dimnames = list(colnames(cor_matr), colnames(cor_matr))) %>% 
        as.data.frame() %>%                              
        rownames_to_column(var = "Variable")
    } %>% 
        flextable() %>%
        autofit() %>% 
        border(border = fp_border_default(color = "black", width = 1), part = "all")

```

```{r, fig.width=14, fig.height=10}

cor_matrix_melt <- cleaned_data %>% 
  select_if(is.numeric) %>% 
  as.matrix() %>% 
  rcorr() %>% 
  {
    cor_matr <- .$r
    melt(cor_matr)  # Плавим корреляционную матрицу для ggplot
  }

ggplot(cor_matrix_melt, aes(Var1, Var2, fill = value)) + 
        geom_tile() + 
        scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                             midpoint = 0, limit = c(-1, 1), space = "Lab", 
                             name = "Correlation") + 
        theme_minimal() +
        theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                         size = 12, hjust = 1),
              axis.text.y = element_text(size = 12)) +
        coord_fixed() +
        labs(title = "Correlation Matrix", x = "", y = "")
```


Корреляционные матрицы помогают быстро визуализировать и оценить связи между переменными в наборе данных. Это может быть полезно при построении статистических моделей (например, регрессионных). Сильная корреляция между независимыми переменными может указывать на мультиколлинеарность, что повлияет на результаты модели.

Плюсы: 
1. Корреляционные анализы проще и быстрее в выполнении, чем более сложные методы анализа, такие как регрессия или анализ вариации.
2.Результаты легко интерпретировать. 

Минусы:
1. Невозможно установить причинно-следственные связи между переменными.
2. Корреляционные коэффициенты могут быть сильно искажены выбросами в данных.

## Моделирование

1) Постройте регрессионную модель для переменной **Category**. Опишите процесс построения

Построим барплот

```{r}
cleaned_data <- cleaned_data %>% 
  select(-Description, -Nutrient_Data_Bank_Number)


cleaned_data %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "variable",
               values_to = "value") %>% 
  ggplot(aes(x = variable, y = value, fill = Category)) +
  geom_bar(stat = "identity", position = "fill") +
  labs(x = "Переменная", y = "Процентное соотношение") +
  theme_minimal() +
  coord_flip()

```

**Разделим данные на тренировочную и тестовую выборки**

```{r}

set.seed(123)
split <- initial_split(cleaned_data, prop = 0.8, strata = Category)

train_data <- split %>% 
  training()

test_data <- split %>% 
  testing()

```

**Обучаем модель**

```{r}

model <- logistic_reg(mixture = double(1), penalty = double(1)) %>%
  set_engine("glmnet") %>%
  set_mode("classification") %>%
  fit(Category ~ ., data = train_data)

# Результаты модели
tidy(model)
```

**Прогнозируем классы и вероятности**

```{r}
pred_class <- predict(model,
                      new_data = test_data,
                      type = "class")

pred_proba <- predict(model,
                      new_data = test_data,
                      type = "prob")

results <- test_data %>% # Объединяем в таблицу results
  select(Category) %>%
  bind_cols(pred_class, pred_proba)
```

**Оцениваем точность модели с помощью accuracy**
```{r}
accuracy(results, truth = Category, estimate = .pred_class)
```

**Настраиваем гиперпараметры**

```{r}
# Определяем логистическую регрессию с настройкой гиперпараметров mixture и penalty
log_reg <- logistic_reg(mixture = tune(), penalty = tune(), engine = "glmnet")

# Создаем сетку для подбора гиперпараметров
grid <- grid_regular(mixture(), penalty(), levels = c(mixture = 4, penalty = 3))

# Создаем объект для хранения сведений о модели 
log_reg_wf <- workflow() %>%
  add_model(log_reg) %>%
  add_formula(Category ~ .)

# Определяем метод перекрестной проверки на тренировочной выборке (5-кратная кросс-валидация)
folds <- vfold_cv(train_data, v = 5)

# Подбраем гиперпараметры
log_reg_tuned <- tune_grid(
  log_reg_wf,
  resamples = folds,
  grid = grid,
  control = control_grid(save_pred = TRUE)
)

# Выбираем лучшие гиперпараметры на основе AUC
select_best(log_reg_tuned, metric = "roc_auc")

```

**Обучаем модель на тренировочной выборке**

```{r}
log_reg_final <- logistic_reg(penalty = 0.0000000001, mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification") %>%
  fit(Category ~., data = train_data)
```

**Оцениваем эффективность модели на тестовой выборке**

```{r}

pred_class <- predict(log_reg_final,
                      new_data = test_data,
                      type = "class")
results <- test_data %>%
  select(Category) %>%
  bind_cols(pred_class, pred_proba)

# Строим матрицу ошибок (confusion matrix) для оценки качества классификации
conf_mat(results, truth = Category,
         estimate = .pred_class)
```


**Создаем таблицу коэффициентов финальной модели, создаем график**

```{r}
coeff <- tidy(log_reg_final) %>% 
  arrange(desc(abs(estimate))) %>% 
  filter(abs(estimate) > 0.5) # абсолютное значение больше 0.5

ggplot(coeff, aes(x = term, y = estimate, fill = term)) + 
  geom_col() + 
  theme_minimal() +
  coord_flip()
```



